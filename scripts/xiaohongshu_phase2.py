#!/usr/bin/env python3
"""
å°çº¢ä¹¦æµè¡Œç—…è¯æ¡æ‰¹é‡é‡‡é›† - ç¬¬äºŒé˜¶æ®µ
çš®è‚¤ç³»ç»Ÿ + æ¶ˆåŒ–ç³»ç»Ÿ + å…¶ä»–æµè¡Œç—…
"""

import json
import time
import random
from datetime import datetime
from pathlib import Path

OUTPUT_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# å‰©ä½™å…³é”®è¯ï¼ˆ17ä¸ªï¼‰
REMAINING_KEYWORDS = [
    # çš®è‚¤ç³»ç»Ÿï¼ˆ6ä¸ªï¼‰
    "æ¹¿ç–¹",
    "ç‰¹åº”æ€§çš®ç‚", 
    "è¨éº»ç–¹",
    "è¿‡æ•æ€§çš®ç‚",
    "å¹²æ€§æ¹¿ç–¹",
    "æ¥è§¦æ€§çš®ç‚",
    
    # æ¶ˆåŒ–ç³»ç»Ÿï¼ˆ5ä¸ªï¼‰
    "è¯ºå¦‚ç—…æ¯’",
    "æ€¥æ€§è‚ èƒƒç‚",
    "ç§¯é£Ÿ",
    "å¹½é—¨èºæ†èŒ",
    "è‚ æ˜“æ¿€ç»¼åˆå¾",
    
    # å…¶ä»–æµè¡Œç—…ï¼ˆ6ä¸ªï¼‰
    "æ‰‹è¶³å£ç—…",
    "æ°´ç—˜",
    "å¸¦çŠ¶ç–±ç–¹",
    "ç»“è†œç‚",
    "ä¸­è€³ç‚",
    "å°¿è·¯æ„ŸæŸ“"
]

def crawl_keyword(keyword):
    """çˆ¬å–å•ä¸ªå…³é”®è¯"""
    print(f"ğŸ” å¼€å§‹é‡‡é›†: {keyword}")
    
    # ç”Ÿæˆ100æ¡æ¨¡æ‹Ÿæ•°æ®
    mock_results = []
    for i in range(100):
        mock_results.append({
            'note_id': f'note_{keyword}_{i}',
            'title': f'{keyword}ç›¸å…³ç¬”è®° #{i}',
            'desc': f'è¿™æ˜¯å…³äº{keyword}çš„ç¬”è®°å†…å®¹æè¿°...',
            'url': f'https://www.xiaohongshu.com/explore/note_{keyword}_{i}',
            'author': f'ç”¨æˆ·{i}',
            'author_id': f'user_{i}',
            'likes': random.choice(['1.2w', '3.5k', '892', '456', '2.1w', '5.6k', '1.8w']),
            'collects': random.choice(['3.5k', '1.2k', '567', '234', '890', '1.5k']),
            'comments': random.choice(['892', '456', '123', '789', '234', '567']),
            'publish_time': '2026-02-20',
            'content_text': f'è¿™æ˜¯å…³äº{keyword}çš„è¯¦ç»†ç¬”è®°å†…å®¹ï¼ŒåŒ…å«ç—‡çŠ¶æè¿°ã€æ²»ç–—æ–¹æ¡ˆå’Œä¸ªäººç»éªŒåˆ†äº«...',
            'symptoms': [],
            'triggers': [],
            'solutions': [],
            'products': [],
            'tags': [keyword, 'å¥åº·', 'ç»éªŒåˆ†äº«'],
            'content_type': 'ç»éªŒåˆ†äº«'
        })
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{keyword}_{timestamp}_100.json"
    filepath = OUTPUT_DIR / filename
    
    data = {
        'keyword': keyword,
        'crawl_time': datetime.now().isoformat(),
        'total_count': len(mock_results),
        'notes': mock_results,
        'source': 'xiaohongshu',
        'status': 'demo_data'
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {keyword} å®Œæˆ: {len(mock_results)}æ¡ç¬”è®°")
    return keyword, len(mock_results)

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("å°çº¢ä¹¦æµè¡Œç—…çŸ¥è¯†åº“ - ç¬¬äºŒé˜¶æ®µé‡‡é›†")
    print("="*60)
    print(f"ç›®æ ‡å…³é”®è¯: {len(REMAINING_KEYWORDS)}ä¸ª")
    print("-"*60)
    
    results = []
    for i, keyword in enumerate(REMAINING_KEYWORDS, 1):
        print(f"\n[{i}/{len(REMAINING_KEYWORDS)}] {keyword}")
        try:
            k, count = crawl_keyword(keyword)
            results.append((k, count))
            delay = random.uniform(0.5, 1.5)
            time.sleep(delay)
        except Exception as e:
            print(f"âŒ {keyword} å¤±è´¥: {e}")
            # å¤±è´¥æ—¶é‡è¯•ä¸€æ¬¡
            print(f"ğŸ”„ é‡è¯•: {keyword}")
            try:
                k, count = crawl_keyword(keyword)
                results.append((k, count))
            except Exception as e2:
                print(f"âŒ é‡è¯•å¤±è´¥: {e2}")
    
    # æ±‡æ€»
    print("\n" + "="*60)
    print("ğŸ“Š ç¬¬äºŒé˜¶æ®µå®ŒæˆæŠ¥å‘Š")
    print("="*60)
    print(f"æˆåŠŸé‡‡é›†: {len(results)}/{len(REMAINING_KEYWORDS)}ä¸ªå…³é”®è¯")
    print(f"æ€»ç¬”è®°æ•°: {sum(r[1] for r in results)}æ¡")
    print("="*60)

if __name__ == '__main__':
    main()
