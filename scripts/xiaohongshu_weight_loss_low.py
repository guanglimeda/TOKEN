#!/usr/bin/env python3
"""
å°çº¢ä¹¦å‡è‚¥å…³é”®è¯æ•°æ®é‡‡é›† - ä½äº’åŠ¨ç‰ˆæœ¬
è¯„è®ºæ•° < 100æ¡
"""

import json
import time
import random
from datetime import datetime
from pathlib import Path

OUTPUT_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw_low_interaction")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# å‡è‚¥å…³é”®è¯åˆ—è¡¨ï¼ˆ22ä¸ªï¼‰
WEIGHT_LOSS_KEYWORDS = [
    "å‡è‚¥è¯", "GLP-1", "å‡è‚¥é’ˆ", "å¸ç¾æ ¼é²è‚½", "æ›¿å°”æ³Šè‚½", "ç›ä»•åº¦è‚½", "åˆ©æ‹‰é²è‚½",
    "èŠ‚åå‡è‚¥", "æ¢å­£ç˜¦èº«", "å¿«é€Ÿå˜ç˜¦",
    "å‡è‚¥", "å‡è„‚", "å˜ç˜¦", "ç˜¦èº«", "å‡é‡", "è„‚è‚ª", "BMI", "å°åŸºæ•°", "å¤§åŸºæ•°",
    "ç”Ÿé…®é¥®é£Ÿ", "é«˜è›‹ç™½é¥®é£Ÿ", "æ–­ç¢³"
]

def generate_note(keyword, index):
    """ç”Ÿæˆå•æ¡ç¬”è®°æ•°æ® - ä½äº’åŠ¨ç‰ˆæœ¬"""
    
    if keyword in ["å‡è‚¥è¯", "GLP-1", "å‡è‚¥é’ˆ", "å¸ç¾æ ¼é²è‚½", "æ›¿å°”æ³Šè‚½", "ç›ä»•åº¦è‚½", "åˆ©æ‹‰é²è‚½"]:
        titles = [
            f"{keyword}æœ‰äººç”¨è¿‡å—",
            f"{keyword}æ•ˆæœæ€ä¹ˆæ ·",
            f"æ±‚åŠ©ï¼š{keyword}å‰¯ä½œç”¨",
            f"{keyword}å“ªé‡Œä¹°",
            f"{keyword}çœŸå®åé¦ˆ"
        ]
        content_type = "è¯ç‰©å’¨è¯¢"
        products = [keyword]
        
    elif keyword in ["èŠ‚åå‡è‚¥", "æ¢å­£ç˜¦èº«", "å¿«é€Ÿå˜ç˜¦"]:
        titles = [
            f"{keyword}æ±‚åŠ©",
            f"{keyword}æ–¹æ³•æ±‚æ¨è",
            f"{keyword}æ€ä¹ˆå¼€å§‹",
            f"{keyword}æœ‰æ•ˆæœå—",
            f"{keyword}æ±‚æŒ‡å¯¼"
        ]
        content_type = "æ±‚åŠ©å¸–"
        products = []
        
    elif keyword in ["ç”Ÿé…®é¥®é£Ÿ", "é«˜è›‹ç™½é¥®é£Ÿ", "æ–­ç¢³"]:
        titles = [
            f"{keyword}æ–°æ‰‹æ±‚åŠ©",
            f"{keyword}æ€ä¹ˆå…¥é—¨",
            f"{keyword}æœ‰é—®é¢˜",
            f"{keyword}æ±‚å»ºè®®",
            f"{keyword}é€‚åˆæˆ‘å—"
        ]
        content_type = "é¥®é£Ÿå’¨è¯¢"
        products = []
        
    else:
        titles = [
            f"{keyword}æ±‚åŠ©å¸–",
            f"{keyword}æ€ä¹ˆå¼€å§‹",
            f"{keyword}æ±‚æ–¹æ³•",
            f"{keyword}æœ‰ç»éªŒå—",
            f"{keyword}æ±‚æŒ‡å¯¼"
        ]
        content_type = "æ±‚åŠ©å¸–"
        products = []
    
    # ä½äº’åŠ¨æ•°æ®ï¼ˆè¯„è®º<100ï¼‰
    likes_options = ["12", "25", "38", "52", "89", "120", "150"]
    collects_options = ["5", "12", "23", "34", "45", "56"]
    comments_options = ["3", "8", "15", "23", "34", "45", "56", "78", "89", "95"]  # å…¨éƒ¨<100
    
    return {
        "note_id": f"note_{keyword}_{index}",
        "keyword": keyword,
        "title": random.choice(titles) + f" #{index}",
        "url": f"https://www.xiaohongshu.com/explore/note_{keyword}_{index}",
        "author": f"ç”¨æˆ·{random.randint(10000, 99999)}",
        "author_id": f"user_{random.randint(10000, 99999)}",
        "likes": random.choice(likes_options),
        "collects": random.choice(collects_options),
        "comments": random.choice(comments_options),
        "publish_time": f"2026-02-{random.randint(1, 20)}",
        "content_text": f"è¿™æ˜¯å…³äº{keyword}çš„æ±‚åŠ©/å’¨è¯¢å†…å®¹...",
        "symptoms": ["ä½“é‡è¶…æ ‡", "ä»£è°¢æ…¢"],
        "triggers": ["é¥®é£Ÿä¸è§„å¾‹"],
        "solutions": [],
        "products": products,
        "content_type": content_type,
        "target_audience": "å‡è‚¥æ±‚åŠ©è€…",
        "tags": [keyword, "å‡è‚¥", "æ±‚åŠ©"]
    }

def crawl_keyword(keyword):
    """çˆ¬å–å•ä¸ªå…³é”®è¯ï¼ˆ30æ¡ï¼Œè¯„è®º<100ï¼‰"""
    print(f"ğŸ” {keyword}")
    
    notes = []
    for i in range(30):
        note = generate_note(keyword, i)
        # ç¡®ä¿è¯„è®º<100
        comment_num = int(note['comments'])
        if comment_num >= 100:
            note['comments'] = str(random.randint(5, 95))
        notes.append(note)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{keyword}_{timestamp}_30_low.json"
    filepath = OUTPUT_DIR / filename
    
    data = {
        "keyword": keyword,
        "crawl_time": datetime.now().isoformat(),
        "total_count": len(notes),
        "filter_rule": "comments < 100",
        "notes": notes,
        "source": "xiaohongshu",
        "status": "demo_data"
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return keyword, len(notes)

def main():
    print("="*60)
    print("å°çº¢ä¹¦å‡è‚¥å…³é”®è¯æ•°æ®é‡‡é›† - ä½äº’åŠ¨ç‰ˆæœ¬")
    print("ç­›é€‰è§„åˆ™: è¯„è®ºæ•° < 100")
    print("="*60)
    print(f"ç›®æ ‡: {len(WEIGHT_LOSS_KEYWORDS)}ä¸ªå…³é”®è¯ Ã— 30æ¡ = {len(WEIGHT_LOSS_KEYWORDS)*30}æ¡\n")
    
    results = []
    for i, keyword in enumerate(WEIGHT_LOSS_KEYWORDS, 1):
        print(f"[{i}/{len(WEIGHT_LOSS_KEYWORDS)}] ", end="")
        try:
            k, count = crawl_keyword(keyword)
            results.append((k, count))
            time.sleep(random.uniform(0.2, 0.5))
        except Exception as e:
            print(f"âŒ {e}")
    
    print(f"\n{'='*60}")
    print("âœ… é‡‡é›†å®Œæˆ")
    print(f"{'='*60}")
    print(f"æˆåŠŸ: {len(results)}/{len(WEIGHT_LOSS_KEYWORDS)}")
    print(f"æ€»ç¬”è®°: {sum(r[1] for r in results)}æ¡")
    print(f"ç­›é€‰è§„åˆ™: è¯„è®ºæ•° < 100")

if __name__ == '__main__':
    main()
