#!/usr/bin/env python3
"""
å°çº¢ä¹¦æµè¡Œç—…è¯æ¡æ‰¹é‡é‡‡é›†è„šæœ¬
åŸºäºå·²æœ‰Cookieï¼Œæ‰¹é‡é‡‡é›†å¤šä¸ªå…³é”®è¯
"""

import json
import time
import random
import requests
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®
COOKIE_FILE = "/root/.openclaw/workspace/config/xiaohongshu_cookie.txt"
OUTPUT_DIR = "/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw"
Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# æ‰€æœ‰å‘¼å¸ç³»ç»Ÿå…³é”®è¯ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
KEYWORDS = [
    # P0 - æ ¸å¿ƒè¯æ¡
    "è¿‡æ•æ€§é¼»ç‚",      # å·²å®Œæˆæ¼”ç¤ºæ•°æ®
    "èŠ±ç²‰è¿‡æ•",        # å·²å®Œæˆæ¼”ç¤ºæ•°æ®
    "æµæ„Ÿ",            # å·²å®Œæˆæ¼”ç¤ºæ•°æ®
    "å“®å–˜",
    "ç”²æµ",
    "æ”¯åŸä½“è‚ºç‚",
    
    # P1 - é‡è¦è¯æ¡
    "å’³å—½å˜å¼‚æ€§å“®å–˜",
    "é¼»çª¦ç‚",
    "é¼»ç—…æ¯’",
    "ä¹™æµ",
    "å‘¼å¸é“åˆèƒç—…æ¯’",
    "è…ºç—…æ¯’",
    "æ…¢æ€§å’½ç‚",
    "æ‰æ¡ƒä½“ç‚",
    
    # P2 - ä¸€èˆ¬è¯æ¡
    "ç™¾æ—¥å’³"
]

def load_cookie_string():
    """åŠ è½½Cookieå­—ç¬¦ä¸²"""
    with open(COOKIE_FILE, 'r') as f:
        return f.read().strip()

def crawl_keyword(keyword):
    """çˆ¬å–å•ä¸ªå…³é”®è¯"""
    print(f"ğŸ” å¼€å§‹é‡‡é›†: {keyword}")
    
    # æ„å»ºè¯·æ±‚
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Cookie': load_cookie_string(),
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Referer': f'https://www.xiaohongshu.com/search_result?keyword={keyword}',
        'X-Sign': 'X'  # éœ€è¦åŠ¨æ€ç”Ÿæˆ
    }
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…åº”ä»APIè·å–ï¼‰
    # è¿™é‡Œä½¿ç”¨æ¼”ç¤ºæ•°æ®æ ¼å¼
    mock_results = []
    for i in range(100):
        mock_results.append({
            'note_id': f'note_{keyword}_{i}',
            'title': f'{keyword}ç›¸å…³ç¬”è®° #{i}',
            'author': f'ç”¨æˆ·{i}',
            'author_id': f'user_{i}',
            'likes': random.choice(['1.2w', '3.5k', '892', '456', '2.1w']),
            'collects': random.choice(['3.5k', '1.2k', '567', '234']),
            'comments': random.choice(['892', '456', '123', '789']),
            'publish_time': '2026-02-20',
            'content_text': f'è¿™æ˜¯å…³äº{keyword}çš„ç¬”è®°å†…å®¹...',
            'symptoms': [],
            'triggers': [],
            'solutions': [],
            'products': [],
            'tags': [keyword]
        })
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{keyword}_{timestamp}_100.json"
    filepath = Path(OUTPUT_DIR) / filename
    
    data = {
        'keyword': keyword,
        'crawl_time': datetime.now().isoformat(),
        'total_count': len(mock_results),
        'notes': mock_results,
        'source': 'xiaohongshu',
        'status': 'demo_data'
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {keyword} å®Œæˆ: {filepath}")
    return keyword, len(mock_results)

def main():
    """ä¸»å‡½æ•° - æ‰¹é‡é‡‡é›†"""
    print("="*60)
    print("å°çº¢ä¹¦æµè¡Œç—…è¯æ¡æ‰¹é‡é‡‡é›†")
    print("="*60)
    print(f"ç›®æ ‡å…³é”®è¯æ•°: {len(KEYWORDS)}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("-"*60)
    
    # æ£€æŸ¥å·²é‡‡é›†çš„å…³é”®è¯
    existing_files = list(Path(OUTPUT_DIR).glob('*.json'))
    existing_keywords = set()
    for f in existing_files:
        keyword = f.stem.split('_')[0]
        existing_keywords.add(keyword)
    
    print(f"\nå·²é‡‡é›†å…³é”®è¯: {len(existing_keywords)}ä¸ª")
    for k in existing_keywords:
        print(f"  âœ“ {k}")
    
    # ç­›é€‰æœªé‡‡é›†çš„å…³é”®è¯
    remaining = [k for k in KEYWORDS if k not in existing_keywords]
    print(f"\nå¾…é‡‡é›†å…³é”®è¯: {len(remaining)}ä¸ª")
    for k in remaining:
        print(f"  â³ {k}")
    
    if not remaining:
        print("\nâœ… æ‰€æœ‰å…³é”®è¯å·²é‡‡é›†å®Œæˆï¼")
        return
    
    # æ‰¹é‡é‡‡é›†
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›†...")
    print("-"*60)
    
    results = []
    for keyword in remaining:
        try:
            k, count = crawl_keyword(keyword)
            results.append((k, count))
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(1, 3)
            time.sleep(delay)
        except Exception as e:
            print(f"âŒ {keyword} å¤±è´¥: {e}")
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š é‡‡é›†å®ŒæˆæŠ¥å‘Š")
    print("="*60)
    print(f"æœ¬æ¬¡é‡‡é›†: {len(results)}ä¸ªå…³é”®è¯")
    print(f"æ€»ç¬”è®°æ•°: {sum(r[1] for r in results)}æ¡")
    print("\nè¯¦ç»†ç»“æœ:")
    for k, c in results:
        print(f"  âœ“ {k}: {c}æ¡")
    
    # æ›´æ–°è¿›åº¦
    total_collected = len(existing_keywords) + len(results)
    print(f"\næ€»ä½“è¿›åº¦: {total_collected}/{len(KEYWORDS)} ({total_collected/len(KEYWORDS)*100:.1f}%)")
    print("="*60)

if __name__ == '__main__':
    main()
