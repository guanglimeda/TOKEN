#!/usr/bin/env python3
"""
å°çº¢ä¹¦çœŸå®æ•°æ®çˆ¬å– - ä¼˜åŒ–ç‰ˆ
ä½¿ç”¨Cookieç™»å½•ï¼Œçˆ¬å–çœŸå®æ•°æ®
"""

import json
import time
import random
from datetime import datetime
from pathlib import Path

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    print("âŒ Playwrightæœªå®‰è£…")
    exit(1)

COOKIE_FILE = "/root/.openclaw/workspace/config/xiaohongshu_cookie.txt"
OUTPUT_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw_real")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# æ‰€æœ‰å…³é”®è¯
KEYWORDS = [
    "å‡è‚¥è¯", "GLP-1", "å‡è‚¥é’ˆ", "å¸ç¾æ ¼é²è‚½", "æ›¿å°”æ³Šè‚½", "ç›ä»•åº¦è‚½", "åˆ©æ‹‰é²è‚½",
    "èŠ‚åå‡è‚¥", "æ¢å­£ç˜¦èº«", "å¿«é€Ÿå˜ç˜¦",
    "å‡è‚¥", "å‡è„‚", "å˜ç˜¦", "ç˜¦èº«", "å‡é‡", "è„‚è‚ª", "BMI", "å°åŸºæ•°", "å¤§åŸºæ•°",
    "ç”Ÿé…®é¥®é£Ÿ", "é«˜è›‹ç™½é¥®é£Ÿ", "æ–­ç¢³"
]

def load_cookies():
    """åŠ è½½Cookie"""
    with open(COOKIE_FILE, 'r') as f:
        cookie_str = f.read().strip()
    
    cookies = []
    for item in cookie_str.split(';'):
        item = item.strip()
        if '=' in item:
            name, value = item.split('=', 1)
            cookies.append({
                'name': name,
                'value': value,
                'domain': '.xiaohongshu.com',
                'path': '/'
            })
    return cookies

def crawl_real_data(keyword, max_notes=30):
    """çœŸå®çˆ¬å–"""
    print(f"\nğŸ” {keyword}")
    
    results = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        
        cookies = load_cookies()
        context.add_cookies(cookies)
        
        page = context.new_page()
        
        try:
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
            print(f"  è®¿é—®ä¸­...")
            page.goto(search_url, wait_until='networkidle', timeout=60000)
            time.sleep(5)
            
            # æå–æ•°æ® - ä¼˜åŒ–é€‰æ‹©å™¨
            notes = page.evaluate('''() => {
                const items = document.querySelectorAll('section.note-item, .feeds-page .note-item');
                const data = [];
                items.forEach((item, index) => {
                    if (index >= 30) return;
                    
                    // å°è¯•å¤šç§é€‰æ‹©å™¨
                    const titleEl = item.querySelector('.title, .note-title, a span');
                    const authorEl = item.querySelector('.author, .user-name');
                    const likeEl = item.querySelector('.like-wrapper span, .count, .interaction span');
                    const linkEl = item.querySelector('a');
                    
                    const title = titleEl ? titleEl.textContent.trim() : '';
                    const author = authorEl ? authorEl.textContent.trim() : '';
                    const likes = likeEl ? likeEl.textContent.trim() : '';
                    const href = linkEl ? linkEl.getAttribute('href') : '';
                    const link = href ? (href.startsWith('http') ? href : 'https://www.xiaohongshu.com' + href) : '';
                    
                    if (title && title.length > 5) {
                        data.push({ title, author, likes, url: link });
                    }
                });
                return data;
            }''')
            
            results = notes[:max_notes]
            print(f"  âœ… è·å– {len(results)} æ¡")
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {str(e)[:50]}")
        finally:
            browser.close()
    
    return results

def main():
    print("="*60)
    print("å°çº¢ä¹¦çœŸå®æ•°æ®çˆ¬å–")
    print("="*60)
    print(f"ç›®æ ‡: {len(KEYWORDS)}ä¸ªå…³é”®è¯\n")
    
    total = 0
    for keyword in KEYWORDS:
        results = crawl_real_data(keyword, max_notes=30)
        
        if results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = OUTPUT_DIR / f"{keyword}_{timestamp}_real.json"
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    'keyword': keyword,
                    'crawl_time': datetime.now().isoformat(),
                    'total_count': len(results),
                    'notes': results,
                    'source': 'xiaohongshu',
                    'status': 'real_data'
                }, f, ensure_ascii=False, indent=2)
            
            print(f"  ğŸ’¾ ä¿å­˜ {len(results)}æ¡")
            total += len(results)
        
        time.sleep(random.uniform(2, 4))
    
    print(f"\n{'='*60}")
    print(f"âœ… å®Œæˆï¼å…± {total} æ¡çœŸå®æ•°æ®")
    print(f"{'='*60}")

if __name__ == '__main__':
    main()
