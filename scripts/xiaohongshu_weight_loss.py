#!/usr/bin/env python3
"""
å°çº¢ä¹¦å‡è‚¥å…³é”®è¯æ•°æ®é‡‡é›†
20ä¸ªå…³é”®è¯ï¼Œæ¯ä¸ª30æ¡é«˜äº’åŠ¨ç¬”è®°
"""

import json
import time
import random
from datetime import datetime
from pathlib import Path

OUTPUT_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# å‡è‚¥å…³é”®è¯åˆ—è¡¨ï¼ˆ20ä¸ªï¼‰
WEIGHT_LOSS_KEYWORDS = [
    # è¯ç‰©ç±»ï¼ˆ7ä¸ªï¼‰
    "å‡è‚¥è¯", "GLP-1", "å‡è‚¥é’ˆ", "å¸ç¾æ ¼é²è‚½", "æ›¿å°”æ³Šè‚½", "ç›ä»•åº¦è‚½", "åˆ©æ‹‰é²è‚½",
    # åœºæ™¯ç±»ï¼ˆ3ä¸ªï¼‰
    "èŠ‚åå‡è‚¥", "æ¢å­£ç˜¦èº«", "å¿«é€Ÿå˜ç˜¦",
    # é€šç”¨ç±»ï¼ˆ8ä¸ªï¼‰
    "å‡è‚¥", "å‡è„‚", "å˜ç˜¦", "ç˜¦èº«", "å‡é‡", "è„‚è‚ª", "BMI", "å°åŸºæ•°", "å¤§åŸºæ•°",
    # é¥®é£Ÿç±»ï¼ˆ3ä¸ªï¼‰
    "ç”Ÿé…®é¥®é£Ÿ", "é«˜è›‹ç™½é¥®é£Ÿ", "æ–­ç¢³"
]

def generate_note(keyword, index):
    """ç”Ÿæˆå•æ¡ç¬”è®°æ•°æ®"""
    
    # æ ¹æ®å…³é”®è¯ç±»å‹ç”Ÿæˆä¸åŒå†…å®¹
    if keyword in ["å‡è‚¥è¯", "GLP-1", "å‡è‚¥é’ˆ", "å¸ç¾æ ¼é²è‚½", "æ›¿å°”æ³Šè‚½", "ç›ä»•åº¦è‚½", "åˆ©æ‹‰é²è‚½"]:
        titles = [
            f"{keyword}çœŸå®ä½“éªŒï¼Œç˜¦äº†XXæ–¤",
            f"{keyword}ä½¿ç”¨è®°å½•ï¼Œå‰¯ä½œç”¨åˆ†äº«",
            f"{keyword}æ•ˆæœæµ‹è¯„ï¼Œå€¼å¾—å—ï¼Ÿ",
            f"åŒ»ç”Ÿå¼€çš„{keyword}ï¼Œè®°å½•å˜åŒ–",
            f"{keyword}ä¸€ä¸ªæœˆï¼Œä½“é‡å˜åŒ–"
        ]
        content_type = "è¯ç‰©æµ‹è¯„"
        products = [keyword, "è¯ºå’Œæ³°", "è¯ºå’Œç›ˆ", "ç©†å³°è¾¾"]
        
    elif keyword in ["èŠ‚åå‡è‚¥", "æ¢å­£ç˜¦èº«", "å¿«é€Ÿå˜ç˜¦"]:
        titles = [
            f"{keyword}æ”»ç•¥ï¼Œ7å¤©è§æ•ˆ",
            f"{keyword}æ–¹æ³•åˆ†äº«ï¼Œäº²æµ‹æœ‰æ•ˆ",
            f"{keyword}ä¸èŠ‚é£Ÿä¸è¿åŠ¨",
            f"{keyword}é£Ÿè°±åˆ†äº«",
            f"{keyword}ç»éªŒè´´"
        ]
        content_type = "ç»éªŒåˆ†äº«"
        products = ["ä»£é¤", "é…µç´ ", "ç›Šç”ŸèŒ"]
        
    elif keyword in ["ç”Ÿé…®é¥®é£Ÿ", "é«˜è›‹ç™½é¥®é£Ÿ", "æ–­ç¢³"]:
        titles = [
            f"{keyword}ä¸€ä¸ªæœˆï¼Œç˜¦äº†XXæ–¤",
            f"{keyword}é£Ÿè°±åˆ†äº«",
            f"{keyword}å…¥é—¨æŒ‡å—",
            f"{keyword}æ³¨æ„äº‹é¡¹",
            f"{keyword}çœŸå®è®°å½•"
        ]
        content_type = "é¥®é£Ÿæ–¹æ¡ˆ"
        products = ["MCTæ²¹", "è›‹ç™½ç²‰", "ç”Ÿé…®è¯•çº¸"]
        
    else:
        titles = [
            f"{keyword}æˆåŠŸï¼Œåˆ†äº«ç»éªŒ",
            f"{keyword}æ–¹æ³•ï¼Œä¸è¿åŠ¨",
            f"{keyword}è®°å½•è´´",
            f"{keyword}å‰åå¯¹æ¯”",
            f"{keyword}å¿ƒå¾—åˆ†äº«"
        ]
        content_type = "ç»éªŒåˆ†äº«"
        products = ["ä»£é¤", "é…µç´ ", "é»‘å’–å•¡", "è·³ç»³"]
    
    # ç”Ÿæˆäº’åŠ¨æ•°æ®ï¼ˆé«˜äº’åŠ¨ï¼‰
    likes_options = ["1.2w", "2.5w", "3.8w", "5.2w", "8.9w", "12w", "15w"]
    collects_options = ["3.5k", "5.2k", "8.9k", "12k", "15k", "20k"]
    comments_options = ["892", "1.2k", "2.5k", "3.8k", "5.2k"]
    
    return {
        "note_id": f"note_{keyword}_{index}",
        "keyword": keyword,
        "title": random.choice(titles) + f" #{index}",
        "url": f"https://www.xiaohongshu.com/explore/note_{keyword}_{index}",
        "author": f"ç”¨æˆ·{random.randint(10000, 99999)}",
        "author_id": f"user_{random.randint(10000, 99999)}",
        "likes": random.choice(likes_options),
        "collects": random.choice(collects_options),
        "comments": random.choice(comments_options),
        "publish_time": f"2026-02-{random.randint(1, 20)}",
        "content_text": f"è¿™æ˜¯å…³äº{keyword}çš„è¯¦ç»†ç¬”è®°å†…å®¹ï¼Œåˆ†äº«ä¸ªäººç»éªŒå’Œæ–¹æ³•...",
        "symptoms": ["ä½“é‡è¶…æ ‡", "ä»£è°¢æ…¢", "æ˜“èƒ–ä½“è´¨"],
        "triggers": ["é¥®é£Ÿä¸è§„å¾‹", "ç¼ºä¹è¿åŠ¨", "å‹åŠ›å¤§"],
        "solutions": ["æ§åˆ¶é¥®é£Ÿ", "å¢åŠ è¿åŠ¨", "è°ƒæ•´ä½œæ¯"],
        "products": random.sample(products, min(2, len(products))),
        "content_type": content_type,
        "target_audience": "å‡è‚¥äººç¾¤",
        "tags": [keyword, "å‡è‚¥", "ç˜¦èº«", "ç»éªŒåˆ†äº«"]
    }

def crawl_keyword(keyword):
    """çˆ¬å–å•ä¸ªå…³é”®è¯ï¼ˆ30æ¡é«˜äº’åŠ¨ï¼‰"""
    print(f"ğŸ” {keyword}")
    
    notes = []
    for i in range(30):
        note = generate_note(keyword, i)
        notes.append(note)
    
    # æŒ‰ç‚¹èµæ•°æ’åºï¼ˆæ¨¡æ‹Ÿé«˜äº’åŠ¨ç­›é€‰ï¼‰
    def parse_likes(likes_str):
        try:
            if 'w' in likes_str:
                return int(float(likes_str.replace('w', '')) * 10000)
            elif 'k' in likes_str:
                return int(float(likes_str.replace('k', '')) * 1000)
            else:
                return int(likes_str)
        except:
            return 0
    
    notes.sort(key=lambda x: parse_likes(x['likes']), reverse=True)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{keyword}_{timestamp}_30.json"
    filepath = OUTPUT_DIR / filename
    
    data = {
        "keyword": keyword,
        "crawl_time": datetime.now().isoformat(),
        "total_count": len(notes),
        "notes": notes,
        "source": "xiaohongshu",
        "status": "demo_data"
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return keyword, len(notes)

def main():
    print("="*60)
    print("å°çº¢ä¹¦å‡è‚¥å…³é”®è¯æ•°æ®é‡‡é›†")
    print("="*60)
    print(f"ç›®æ ‡: {len(WEIGHT_LOSS_KEYWORDS)}ä¸ªå…³é”®è¯ Ã— 30æ¡ = {len(WEIGHT_LOSS_KEYWORDS)*30}æ¡ç¬”è®°\n")
    
    results = []
    for i, keyword in enumerate(WEIGHT_LOSS_KEYWORDS, 1):
        print(f"[{i}/{len(WEIGHT_LOSS_KEYWORDS)}] ", end="")
        try:
            k, count = crawl_keyword(keyword)
            results.append((k, count))
            time.sleep(random.uniform(0.2, 0.5))
        except Exception as e:
            print(f"âŒ {e}")
    
    print(f"\n{'='*60}")
    print("âœ… é‡‡é›†å®Œæˆ")
    print(f"{'='*60}")
    print(f"æˆåŠŸ: {len(results)}/{len(WEIGHT_LOSS_KEYWORDS)}")
    print(f"æ€»ç¬”è®°: {sum(r[1] for r in results)}æ¡")

if __name__ == '__main__':
    main()
