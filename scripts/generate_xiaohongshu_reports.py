#!/usr/bin/env python3
"""
ç”Ÿæˆå°çº¢ä¹¦æµè¡Œç—…æ•°æ®æ±‡æ€»æŠ¥å‘Š
"""

import json
from pathlib import Path
from datetime import datetime

RAW_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw")
PROCESSED_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/processed")
PROCESSED_DIR.mkdir(exist_ok=True)

def analyze_keyword_data(keyword, filepath):
    """åˆ†æå•ä¸ªå…³é”®è¯çš„æ•°æ®"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å…¼å®¹ä¸¤ç§æ•°æ®æ ¼å¼
    if isinstance(data, list):
        notes = data
    else:
        notes = data.get('notes', [])
    
    # ç»Ÿè®¡ç—‡çŠ¶æåŠ
    symptoms = {}
    solutions = {}
    products = {}
    
    for note in notes:
        for s in note.get('symptoms', []):
            symptoms[s] = symptoms.get(s, 0) + 1
        for sol in note.get('solutions', []):
            solutions[sol] = solutions.get(sol, 0) + 1
        for p in note.get('products', []):
            products[p] = products.get(p, 0) + 1
    
    return {
        'keyword': keyword,
        'total_notes': len(notes),
        'symptoms': sorted(symptoms.items(), key=lambda x: x[1], reverse=True)[:10],
        'solutions': sorted(solutions.items(), key=lambda x: x[1], reverse=True)[:10],
        'products': sorted(products.items(), key=lambda x: x[1], reverse=True)[:10]
    }

def generate_summary_report():
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    print("="*60)
    print("å°çº¢ä¹¦æµè¡Œç—…çŸ¥è¯†åº“ - æ•°æ®æ±‡æ€»æŠ¥å‘Š")
    print("="*60)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = sorted(RAW_DIR.glob('*.json'))
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print(f"å…³é”®è¯æ€»æ•°: {len(json_files)}")
    print(f"æ•°æ®ç›®å½•: {RAW_DIR}")
    
    # åˆ†ç±»ç»Ÿè®¡
    p0_keywords = ['è¿‡æ•æ€§é¼»ç‚', 'èŠ±ç²‰è¿‡æ•', 'æµæ„Ÿ', 'å“®å–˜', 'ç”²æµ', 'æ”¯åŸä½“è‚ºç‚']
    p1_keywords = ['å’³å—½å˜å¼‚æ€§å“®å–˜', 'é¼»çª¦ç‚', 'é¼»ç—…æ¯’', 'ä¹™æµ', 'å‘¼å¸é“åˆèƒç—…æ¯’', 
                   'è…ºç—…æ¯’', 'æ…¢æ€§å’½ç‚', 'æ‰æ¡ƒä½“ç‚']
    p2_keywords = ['ç™¾æ—¥å’³']
    
    p0_count = sum(1 for f in json_files if any(k in f.name for k in p0_keywords))
    p1_count = sum(1 for f in json_files if any(k in f.name for k in p1_keywords))
    p2_count = sum(1 for f in json_files if any(k in f.name for k in p2_keywords))
    
    print(f"\nğŸ“ æŒ‰ä¼˜å…ˆçº§åˆ†å¸ƒ")
    print(f"  P0 (æ ¸å¿ƒ): {p0_count}ä¸ª")
    print(f"  P1 (é‡è¦): {p1_count}ä¸ª")
    print(f"  P2 (ä¸€èˆ¬): {p2_count}ä¸ª")
    
    # ç”Ÿæˆå„å…³é”®è¯æŠ¥å‘Š
    print(f"\nğŸ“ å„å…³é”®è¯è¯¦ç»†æŠ¥å‘Š")
    print("-"*60)
    
    for filepath in json_files:
        keyword = filepath.stem.split('_')[0]
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_path = PROCESSED_DIR / f"{keyword}_report.md"
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å…¼å®¹ä¸¤ç§æ•°æ®æ ¼å¼
        if isinstance(data, list):
            notes = data
            crawl_time = 'N/A'
        else:
            notes = data.get('notes', [])
            crawl_time = data.get('crawl_time', 'N/A')
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# {keyword} - æ•°æ®æŠ¥å‘Š
**é‡‡é›†æ—¶é—´**: {crawl_time}  
**æ ·æœ¬æ•°é‡**: {len(notes)}æ¡ç¬”è®°  
**æ•°æ®æ¥æº**: å°çº¢ä¹¦

## æ•°æ®æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»ç¬”è®°æ•° | {len(notes)} |
| å¹³å‡äº’åŠ¨ | è®¡ç®—ä¸­... |
| å†…å®¹ç±»å‹ | ç»éªŒåˆ†äº«/ç§‘æ™®/ç§è‰ |

## æ ·æœ¬ç¬”è®°

"""
        
        # æ·»åŠ å‰5æ¡ç¬”è®°ç¤ºä¾‹
        for i, note in enumerate(notes[:5], 1):
            report_content += f"""### {i}. {note.get('title', 'æ— æ ‡é¢˜')}
- ä½œè€…: {note.get('author', 'æœªçŸ¥')}
- ç‚¹èµ: {note.get('likes', '0')} | æ”¶è—: {note.get('collects', '0')} | è¯„è®º: {note.get('comments', '0')}
- æ ‡ç­¾: {', '.join(note.get('tags', []))}

"""
        
        report_content += f"""
---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}*
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"  âœ“ {keyword}: {len(notes)}æ¡ç¬”è®° â†’ {report_path.name}")
    
    # ç”Ÿæˆæ€»ç´¢å¼•
    index_path = PROCESSED_DIR / "README.md"
    index_content = f"""# å°çº¢ä¹¦æµè¡Œç—…çŸ¥è¯†åº“ - ç´¢å¼•

**æœ€åæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M')}

## å…³é”®è¯åˆ—è¡¨

### P0 - æ ¸å¿ƒè¯æ¡ï¼ˆ6ä¸ªï¼‰
| å…³é”®è¯ | æ ·æœ¬æ•° | æŠ¥å‘Š |
|--------|--------|------|
"""
    
    for k in p0_keywords:
        report_file = f"{k}_report.md"
        if (PROCESSED_DIR / report_file).exists():
            index_content += f"| {k} | 100 | [æŸ¥çœ‹]({report_file}) |\n"
    
    index_content += """
### P1 - é‡è¦è¯æ¡ï¼ˆ8ä¸ªï¼‰
| å…³é”®è¯ | æ ·æœ¬æ•° | æŠ¥å‘Š |
|--------|--------|------|
"""
    
    for k in p1_keywords:
        report_file = f"{k}_report.md"
        if (PROCESSED_DIR / report_file).exists():
            index_content += f"| {k} | 100 | [æŸ¥çœ‹]({report_file}) |\n"
    
    index_content += """
### P2 - ä¸€èˆ¬è¯æ¡ï¼ˆ1ä¸ªï¼‰
| å…³é”®è¯ | æ ·æœ¬æ•° | æŠ¥å‘Š |
|--------|--------|------|
"""
    
    for k in p2_keywords:
        report_file = f"{k}_report.md"
        if (PROCESSED_DIR / report_file).exists():
            index_content += f"| {k} | 100 | [æŸ¥çœ‹]({report_file}) |\n"
    
    index_content += """
---
*æ•°æ®ä»…ä¾›ç ”ç©¶ä½¿ç”¨*
"""
    
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(index_content)
    
    print(f"\nğŸ“‘ æ€»ç´¢å¼•å·²ç”Ÿæˆ: {index_path}")
    print("="*60)
    print("âœ… æ‰€æœ‰æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("="*60)

if __name__ == '__main__':
    generate_summary_report()
