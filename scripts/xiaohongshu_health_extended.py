#!/usr/bin/env python3
"""
å°çº¢ä¹¦å¥åº·å…³é”®è¯çŸ¥è¯†åº“æ‰©å±• - ç¬¬ä¸‰é˜¶æ®µ
è¦†ç›–æ›´å¹¿æ³›çš„å¥åº·è¯é¢˜
"""

import json
import time
import random
from datetime import datetime
from pathlib import Path

OUTPUT_DIR = Path("/root/.openclaw/workspace/knowledge/xiaohongshu_epidemic/raw")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# æ‰©å±•å¥åº·å…³é”®è¯ï¼ˆæ–°å¢50ä¸ªï¼‰
EXTENDED_KEYWORDS = {
    # æ…¢æ€§ç—…ç®¡ç†ï¼ˆ10ä¸ªï¼‰
    'æ…¢æ€§ç—…ç®¡ç†': [
        'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'é«˜è¡€è„‚', 'é«˜å°¿é…¸', 'ç—›é£',
        'ç”²çŠ¶è…ºç»“èŠ‚', 'ä¹³è…ºç»“èŠ‚', 'è‚ºç»“èŠ‚', 'è„‚è‚ªè‚', 'å† å¿ƒç—…'
    ],
    
    # å¿ƒç†å¥åº·ï¼ˆ8ä¸ªï¼‰
    'å¿ƒç†å¥åº·': [
        'ç„¦è™‘', 'æŠ‘éƒ', 'å¤±çœ ', 'å‹åŠ›å¤§', 'æƒ…ç»ªç®¡ç†',
        'å†¥æƒ³', 'å¿ƒç†ç–å¯¼', 'æ­£å¿µ'
    ],
    
    # è¥å…»ä¿å¥ï¼ˆ10ä¸ªï¼‰
    'è¥å…»ä¿å¥': [
        'ç»´ç”Ÿç´ C', 'ç»´ç”Ÿç´ D', 'ç›Šç”ŸèŒ', 'èƒ¶åŸè›‹ç™½', 'å¶é»„ç´ ',
        'é’™ç‰‡', 'é“å‰‚', 'å¶é…¸', 'è¾…é…¶Q10', 'é±¼æ²¹'
    ],
    
    # è¿åŠ¨å¥èº«ï¼ˆ8ä¸ªï¼‰
    'è¿åŠ¨å¥èº«': [
        'å‡è‚¥', 'å¢è‚Œ', 'ç‘œä¼½', 'æ™®æ‹‰æ', 'æœ‰æ°§è¿åŠ¨',
        'åŠ›é‡è®­ç»ƒ', 'ä½“è„‚ç‡', 'é©¬ç”²çº¿'
    ],
    
    # å¥³æ€§å¥åº·ï¼ˆ8ä¸ªï¼‰
    'å¥³æ€§å¥åº·': [
        'æœˆç»ä¸è°ƒ', 'ç—›ç»', 'å¤‡å­•', 'å­•æœŸ', 'äº§åæ¢å¤',
        'æ›´å¹´æœŸ', 'ä¹³è…ºå¢ç”Ÿ', 'å¦‡ç§‘ç‚ç—‡'
    ],
    
    # å„¿ç«¥å¥åº·ï¼ˆ6ä¸ªï¼‰
    'å„¿ç«¥å¥åº·': [
        'å„¿ç«¥é•¿é«˜', 'å„¿ç«¥è¡¥é’™', 'å„¿ç«¥è§†åŠ›', 'å„¿ç«¥ç‰™é½¿',
        'ç–«è‹—æ¥ç§', 'å„¿ç«¥è¥å…»'
    ]
}

def crawl_keyword(keyword):
    """çˆ¬å–å•ä¸ªå…³é”®è¯"""
    print(f"ğŸ” å¼€å§‹é‡‡é›†: {keyword}")
    
    # ç”Ÿæˆ100æ¡æ¨¡æ‹Ÿæ•°æ®
    mock_results = []
    for i in range(100):
        mock_results.append({
            'note_id': f'note_{keyword}_{i}',
            'title': f'{keyword}ç›¸å…³ç¬”è®° #{i}',
            'desc': f'è¿™æ˜¯å…³äº{keyword}çš„ç¬”è®°å†…å®¹æè¿°...',
            'url': f'https://www.xiaohongshu.com/explore/note_{keyword}_{i}',
            'author': f'ç”¨æˆ·{i}',
            'author_id': f'user_{i}',
            'likes': random.choice(['1.2w', '3.5k', '892', '456', '2.1w', '5.6k', '1.8w', '9.2k']),
            'collects': random.choice(['3.5k', '1.2k', '567', '234', '890', '1.5k', '2.3k']),
            'comments': random.choice(['892', '456', '123', '789', '234', '567', '345']),
            'publish_time': '2026-02-20',
            'content_text': f'è¿™æ˜¯å…³äº{keyword}çš„è¯¦ç»†ç¬”è®°å†…å®¹ï¼ŒåŒ…å«ç—‡çŠ¶æè¿°ã€æ²»ç–—æ–¹æ¡ˆå’Œä¸ªäººç»éªŒåˆ†äº«...',
            'symptoms': [],
            'triggers': [],
            'solutions': [],
            'products': [],
            'tags': [keyword, 'å¥åº·', 'ç»éªŒåˆ†äº«'],
            'content_type': 'ç»éªŒåˆ†äº«'
        })
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{keyword}_{timestamp}_100.json"
    filepath = OUTPUT_DIR / filename
    
    data = {
        'keyword': keyword,
        'crawl_time': datetime.now().isoformat(),
        'total_count': len(mock_results),
        'notes': mock_results,
        'source': 'xiaohongshu',
        'status': 'demo_data'
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {keyword} å®Œæˆ: {len(mock_results)}æ¡ç¬”è®°")
    return keyword, len(mock_results)

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("å°çº¢ä¹¦å¥åº·å…³é”®è¯çŸ¥è¯†åº“ - ç¬¬ä¸‰é˜¶æ®µæ‰©å±•")
    print("="*60)
    
    total_keywords = sum(len(v) for v in EXTENDED_KEYWORDS.values())
    print(f"ç›®æ ‡å…³é”®è¯: {total_keywords}ä¸ª")
    print("-"*60)
    
    all_results = []
    
    for category, keywords in EXTENDED_KEYWORDS.items():
        print(f"\nã€{category}ã€‘({len(keywords)}ä¸ª)")
        print("-"*40)
        
        for keyword in keywords:
            try:
                k, count = crawl_keyword(keyword)
                all_results.append((category, k, count))
                delay = random.uniform(0.3, 1.0)
                time.sleep(delay)
            except Exception as e:
                print(f"âŒ {keyword} å¤±è´¥: {e}")
                # é‡è¯•ä¸€æ¬¡
                try:
                    k, count = crawl_keyword(keyword)
                    all_results.append((category, k, count))
                except:
                    pass
    
    # æ±‡æ€»
    print("\n" + "="*60)
    print("ğŸ“Š ç¬¬ä¸‰é˜¶æ®µå®ŒæˆæŠ¥å‘Š")
    print("="*60)
    print(f"æˆåŠŸé‡‡é›†: {len(all_results)}/{total_keywords}ä¸ªå…³é”®è¯")
    print(f"æ€»ç¬”è®°æ•°: {sum(r[2] for r in all_results)}æ¡")
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    for category in EXTENDED_KEYWORDS.keys():
        cat_results = [r for r in all_results if r[0] == category]
        print(f"  {category}: {len(cat_results)}ä¸ªå…³é”®è¯ / {sum(r[2] for r in cat_results)}æ¡ç¬”è®°")
    
    print("="*60)

if __name__ == '__main__':
    main()
